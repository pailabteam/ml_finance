{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Networks for Peer Group Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates the application of Siamese networks to construct a peer group scoring used to create peer groups for structured products. It shows a sample implementation to reproduce results from [this presentation](siamese.pdf) (example 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is checked in as a comprezed zip folder. Therefore, we have to unzip it if we run this notebook for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('.') #unzip into current folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is organized "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data, vols, data_dir):\n",
    "    positive = None\n",
    "    negative = None\n",
    "    anchor = None\n",
    "    info = None\n",
    "    distance = None\n",
    "\n",
    "    for vol in vols:\n",
    "        for d in data:\n",
    "            file_prefix = data_dir+d+'_' + str(vol)\n",
    "            if positive is None:\n",
    "                positive=pd.read_csv(file_prefix+'_positive.csv')#, index_col = 0))\n",
    "                negative=pd.read_csv(file_prefix+'_negative.csv')#, index_col = 0))\n",
    "                anchor=pd.read_csv(file_prefix+'_anchor.csv')#, index_col = 0))\n",
    "                info=pd.read_csv(file_prefix+'_info.csv')#, index_col = 0))\n",
    "                distance=pd.read_csv(file_prefix+'_distance.csv')#, index_col = 0))\n",
    "            else:\n",
    "                positive = positive.append(pd.read_csv(file_prefix+'_positive.csv'), ignore_index = True)#, index_col = 0))\n",
    "                negative = negative.append(pd.read_csv(file_prefix+'_negative.csv'), ignore_index = True)#, index_col = 0))\n",
    "                anchor = anchor.append(pd.read_csv(file_prefix+'_anchor.csv'), ignore_index = True)#, index_col = 0))\n",
    "                info = info.append(pd.read_csv(file_prefix+'_info.csv'), ignore_index = True)#, index_col = 0))\n",
    "                distance=distance.append(pd.read_csv(file_prefix+'_distance.csv'), ignore_index = True)#, index_col = 0))\n",
    "    return anchor, positive, negative, info, distance\n",
    "\n",
    "def get_training_data(data, vols, data_dir, shuffle = True, frac=1):\n",
    "    anchor, positive, negative, info, distance = get_data(data, vols , data_dir)\n",
    "    #now shuffle data\n",
    "    if shuffle:\n",
    "        anchor = anchor.sample(frac=frac)\n",
    "        positive = positive.loc[anchor.index]\n",
    "        negative = negative.loc[anchor.index]\n",
    "        info = info.loc[anchor.index]\n",
    "        distance = distance.loc[anchor.index]\n",
    "    x_train=[anchor.values, positive.values, negative.values]\n",
    "    y_train = distance.values\n",
    "    return x_train, y_train, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, info = get_training_data(['EUROPEAN_P1_P1_P1','EUROPEAN_P1_P1_P2'], \n",
    "                                                   vols=[10,15,20,25,30], data_dir = './data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Network with the triplet loss $\\alpha$ function\n",
    "Define custom inputs including the number of layers and nodes, the loss function, the data/contraints and vector of volatilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "inputs = {\n",
    "          'loss': siamese.SiameseNN._triplet_loss_alpha,\n",
    "          'activation':'elu',\n",
    "          'data':['EUROPEAN_P1_P1_P1','EUROPEAN_P1_P1_P2','BUTTERFLY_BUTTERFLY_BUTTERFLY'], \n",
    "          'vols' : [10, 15, 20, 25, 30],\n",
    "          'normalize_output':False,\n",
    "          'kernel_regularizer': None,\n",
    "          'bias_regularizer': None,\n",
    "          'optimizer':keras.optimizers.Adam(lr=0.00035, beta_1=0.9, beta_2=0.999),\n",
    "          'log_dir':'C:\\\\temp\\\\siamese_alpha\\\\',\n",
    "          'train_data_dir':'C:\\\\Users\\\\cwerner.FAV\\\\dev\\\\fav\\\\analytics_tools\\\\python\\\\notebooks\\\\fav\\\\peer_group\\\\data\\\\'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the training data as configured within the above inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and compile the Siamese Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "All functions to create the network with tensorflow/keras are encapsulated in the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     5,
     38,
     53,
     66,
     97
    ]
   },
   "outputs": [],
   "source": [
    "class SiameseNN:\n",
    "    \"\"\"This class encasulates all functions needed to create the siamese network for peer group scoring\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def triplet_loss_alpha(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Implementation of the triplet loss function where alpha is taken from y_true\n",
    "        Arguments:\n",
    "        y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "        y_pred -- python list containing three objects:\n",
    "                anchor -- the encodings for the anchor data\n",
    "                positive -- the encodings for the positive data (similar to anchor)\n",
    "                negative -- the encodings for the negative data (different from anchor)\n",
    "        Returns:\n",
    "        loss -- real number, value of the loss\n",
    "\n",
    "        $$L=\\max(d(A,P)-d(A,N)+\\alpha,0)$$\n",
    "        \"\"\"\n",
    "        total_lenght = y_pred.shape[-1]\n",
    "\n",
    "        anchor = y_pred[:,0:int(total_lenght/3)]\n",
    "        positive = y_pred[:,int(total_lenght/3):int(total_lenght*2/3)]\n",
    "        negative = y_pred[:,int((total_lenght*2)/3):int(total_lenght)]\n",
    "\n",
    "        # distance between the anchor and the positive\n",
    "        pos_dist = keras.backend.sum(keras.backend.square(anchor-positive),axis=1)\n",
    "\n",
    "        # distance between the anchor and the negative\n",
    "        neg_dist = keras.backend.sum(keras.backend.square(anchor-negative),axis=1)\n",
    "\n",
    "        # compute loss\n",
    "        #return keras.backend.mean(keras.backend.maximum(pos_dist-neg_dist+0.1,0.0))\n",
    "        #print(keras.backend.mean(depp.shape))\n",
    "        #print(keras.backend.mean(y_pred[:,3]))\n",
    "        return keras.backend.maximum(pos_dist-neg_dist+0.5*y_true[:,3],0.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(file):\n",
    "        \"\"\"Load a model from file.\n",
    "\n",
    "        Thismethod simply uses the keras built-in method keras.models.load_model where the specialized loss function is specified in custom_objects\n",
    "\n",
    "        Args:\n",
    "            file (str): The filename of fil containing model\n",
    "\n",
    "        Returns:\n",
    "            tensorflow model: The siamese network\n",
    "        \"\"\"\n",
    "        #if newest_subdir is None:\n",
    "        return keras.models.load_model(file, custom_objects={'triplet_loss_alpha': SiameseNN.triplet_loss_alpha})\n",
    "\n",
    "    @staticmethod\n",
    "    def get_submodel(m):\n",
    "        \"\"\"Return the inner model of the siamese network\n",
    "\n",
    "        Args:\n",
    "            m (keras model): The siamese network\n",
    "\n",
    "        Returns:\n",
    "            [keras model]: The inner network of the siamese model\n",
    "        \"\"\"\n",
    "        return m.get_layer(name='distance_model')\n",
    "         \n",
    "    @staticmethod\n",
    "    def _create_simple_network(n_neurons, activation='relu',\n",
    "                        kernel_regularizer=None, bias_regularizer=None, \n",
    "                        input_dim=1, \n",
    "                        normalize_output=False):\n",
    "        \"\"\"Create the network model used as inner model of the siamese network\n",
    "\n",
    "            Args:\n",
    "                n_neurons (int): Number of neurons\n",
    "                activation (str, optional): The keras activation function used in the inner network. Defaults to 'relu'.\n",
    "                kernel_regularizer (regularizer from keras.regularizers, optional): A possible kernel_regularizer. Defaults to None.\n",
    "                bias_regularizer (regularizer from keras.regularizers, optional): A bias regularizer. Defaults to None.\n",
    "                input_dim (int, optional): Input dimension. Defaults to 1.\n",
    "                normalize_output (bool, optional): Flag that determines if the output is normalized. Defaults to False.\n",
    "\n",
    "            Returns:\n",
    "                [model from keras.models]: The resulting inner network\n",
    "        \"\"\"\n",
    "        keras.backend.clear_session()\n",
    "        np.random.seed(42)\n",
    "        model = keras.models.Sequential(name='distance_model')\n",
    "        model.add(keras.layers.Dense(n_neurons[0], activation=activation, input_dim=input_dim, kernel_regularizer=kernel_regularizer, \n",
    "                        bias_regularizer=bias_regularizer, name='distance_model_layer_0')) \n",
    "        for i,n in enumerate(n_neurons[1:]):\n",
    "            name='distance_model_layer_'+str(i+1)\n",
    "            model.add(keras.layers.Dense(n, activation=activation, kernel_regularizer=kernel_regularizer, \n",
    "                        bias_regularizer=bias_regularizer,  name=name)) \n",
    "        if normalize_output:\n",
    "            model.add(keras.layers.Lambda(lambda t: keras.backend.l2_normalize(t, axis=1)))\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def create(n_neurons, activation='relu', kernel_regularizer=None,\n",
    "            bias_regularizer=None, input_dim=1, optimizer = None, \n",
    "            normalize_output = False):\n",
    "        \"\"\"If optimizer is not None, it also directly compiles the model\n",
    "        \"\"\"\n",
    "        model = SiameseNN._create_simple_network(n_neurons, activation=activation, \n",
    "                kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer, \n",
    "                input_dim=input_dim, normalize_output=normalize_output)\n",
    "        input_dim = model.layers[0].input_shape[1]\n",
    "        # Define the tensors for the three input images\n",
    "        anchor_input = keras.layers.Input((input_dim, ), name=\"anchor_input\")\n",
    "        positive_input = keras.layers.Input((input_dim, ), name=\"positive_input\")\n",
    "        negative_input = keras.layers.Input((input_dim, ), name=\"negative_input\") \n",
    "        \n",
    "        # Generate the encodings (feature vectors) for the three images\n",
    "        encoded_a = model(anchor_input)\n",
    "        encoded_p = model(positive_input)\n",
    "        encoded_n = model(negative_input)\n",
    "        \n",
    "        merged_vector = keras.layers.concatenate([encoded_a, encoded_p, encoded_n], axis=-1, name='merged_layer')\n",
    "        #merged_vector = keras.backend.stack([encoded_a, encoded_p, encoded_n])\n",
    "        # Connect the inputs with the outputs\n",
    "        siamese = keras.models.Model(inputs=[anchor_input,positive_input,negative_input],outputs=merged_vector, name='siamese')\n",
    "        if optimizer is not None:\n",
    "            siamese.compile(loss=SiameseNN._triplet_loss, optimizer=optimizer)\n",
    "        return siamese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Siamese Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code cell we define a new Siamese network with\n",
    "- Three layers (15 neurons in the first layer, 10 in the second and 3 in the output layer)\n",
    "- As activation functions we use elu\n",
    "- We do not apply any regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = siamese.SiameseNN.create(n_neurons=(15,10,3,), \n",
    "                                 activation='elu',\n",
    "                                 kernel_regularizer=None, \n",
    "                                 bias_regularizer=None, \n",
    "                                 input_dim =x_train[0].shape[1], \n",
    "                                 normalize_output = False)\n",
    "\n",
    "model.compile(loss=siamese.SiameseNN.triplet_loss_alpha, \n",
    "              optimizer = keras.optimizers.Adam(lr=0.00035, beta_1=0.9, beta_2=0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"siamese\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_input (InputLayer)       [(None, 33)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_input (InputLayer)     [(None, 33)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_input (InputLayer)     [(None, 33)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "distance_model (Sequential)     (None, 3)            703         anchor_input[0][0]               \n",
      "                                                                 positive_input[0][0]             \n",
      "                                                                 negative_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "merged_layer (Concatenate)      (None, 9)            0           distance_model[1][0]             \n",
      "                                                                 distance_model[2][0]             \n",
      "                                                                 distance_model[3][0]             \n",
      "==================================================================================================\n",
      "Total params: 703\n",
      "Trainable params: 703\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'OSError' object has no attribute 'message'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1028\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m   1007\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[1;32m-> 1008\u001b[1;33m                     signatures, options)\n\u001b[0m\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m    111\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[1;32m--> 112\u001b[1;33m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[0;32m    113\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myenv\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myenv\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to create file (unable to open file: name = 'C:\\temp\\siamese_alpha\\\\20200505-221611\\best_model.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-370b2e7baeff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                     validation_split=0.2)\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m#save model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myenv\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1043\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m         \u001b[1;31m# `e.errno` appears to be `None` so checking the content of `e.message`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1045\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;34m'is a directory'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1046\u001b[0m           raise IOError('Please specify a non-directory filepath for '\n\u001b[0;32m   1047\u001b[0m                         \u001b[1;34m'ModelCheckpoint. Filepath used is an existing '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OSError' object has no attribute 'message'"
     ]
    }
   ],
   "source": [
    "cb = [keras.callbacks.TensorBoard(profile_batch=0, log_dir=inputs['log_dir'], histogram_freq=100),]\n",
    "cb.append(keras.callbacks.ModelCheckpoint(inputs['log_dir']+'\\\\' + dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '\\\\best_model.h5', save_best_only = True))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=2000,\n",
    "                    verbose=0,\n",
    "                    callbacks=cb,\n",
    "                    validation_split=0.2)\n",
    "#save model\n",
    "model.reset_metrics()\n",
    "model.save(inputs['log_dir'] +'\\\\model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'History' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-c220dbbe221f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'History' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "history[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "102.898px",
    "width": "251.989px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
